{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Into the RLverse: Value iteration algorithm\n",
    " Goal: Implementing the algorithms I have learnt to find optimal policies as well as the value functions for the given MDPs (Value Iteration - a special case of Policy Iteration).\n",
    "\n",
    " Note: Instructions for running this code are given at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Importing the necessary libraries and specifying the engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Defining the value iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P is the MDP, gamma is the discount factor, theta is the convergence-checking threshold\n",
    "def value_iteration(P, n_states, n_actions, gamma, theta=1e-10):\n",
    "\n",
    "    # initialising the state-value function (V-function)\n",
    "    V = np.zeros(n_states, dtype=np.float64)\n",
    "\n",
    "    while True: # iterates until the value function converges (which is checked for later)\n",
    "\n",
    "        # initialising the action-value function (Q-function)\n",
    "        # it is a 2D array because each state has multiple possible actions, each with it's own Q-value\n",
    "        # no. of rows = no. of states, no. of columns = no. of actions\n",
    "        Q = np.zeros((n_states, n_actions), dtype=np.float64)\n",
    "\n",
    "        # now running the loop to calculate the updated Q-function for every\n",
    "        # transition of every action of every state:\n",
    "        for index, (_, initial, action, final, reward, prob) in P.iterrows():\n",
    "                \n",
    "            # calculating the action-value function\n",
    "            Q[initial][action] += prob*(reward + gamma*V[final])\n",
    "\n",
    "        # if function converges, break\n",
    "        if np.max(np.abs(V - np.max(Q, axis=1))) < theta:\n",
    "            iterate=False\n",
    "            break\n",
    "\n",
    "        # else, update the state-value function (Policy Improvement)\n",
    "        # assign each state the value of the maximum returning function in that state\n",
    "        V = np.max(Q, axis=1)\n",
    "\n",
    "    # once the value function has converged, calculate the optimal policy pi\n",
    "    # here it is a function (a lambda) that returns the optimal action for each state\n",
    "    pi = lambda s: {s:a for s,a in enumerate(np.argmax(Q, axis=1))} [s]\n",
    "\n",
    "    # return the optimal state-value function and the optimal policy\n",
    "    return pi, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Assigning the path to the text file containing the MDP, and the file to write output to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"C:/Users/eshaa/Desktop/WiDS-RLverse/mdp-10-5.txt\"\n",
    "output = \"C:/Users/eshaa/Desktop/WiDS-RLverse/vi_output.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Extracting parameters from the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input, 'r') as file:\n",
    "\n",
    "    for line in file:\n",
    "\n",
    "        if line.startswith('states'):\n",
    "            n_states = int(line.split()[1])\n",
    "\n",
    "        elif line.startswith('actions'):\n",
    "            n_actions = int(line.split()[1])\n",
    "\n",
    "        elif line.startswith('gamma'):\n",
    "            gamma = float(line.split()[1])\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Reading the MDP data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eshaa\\AppData\\Local\\Temp\\ipykernel_14016\\3956382546.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  MDP = pd.read_table(input, sep=' ', names=(\"_\", \"initial state\", \"action taken\", \"final state\", \"reward\", \"transition probability\"), skiprows=2, skipfooter=1)\n"
     ]
    }
   ],
   "source": [
    "MDP = pd.read_table(input, sep=' ', names=(\"_\", \"initial state\", \"action taken\", \"final state\", \"reward\", \"transition probability\"), skiprows=2, skipfooter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Running the value-iteration algorithm on the MDP to obtain the optimal policy and optimal state-value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, V = value_iteration(MDP, n_states, n_actions, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Writing the output to the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output, 'w') as file:\n",
    "\n",
    "    for state in range(n_states):\n",
    "\n",
    "        file.write(f'{V[state]:.6f} {pi(state)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions for running the above code:**\n",
    "\n",
    "- Replace addresses in step 5 with the paths to your input and output files.\n",
    "\n",
    "- Input text file should contain the MDP data in the format:\n",
    "\n",
    "    states &lt; number of states &gt;\\\n",
    "    actions &lt; number of actions &gt;\\\n",
    "    tran &lt; initial state &gt;  &lt; action taken &gt;  &lt; final state &gt;  &lt; reward &gt;  &lt; transition probability &gt;\\\n",
    "    ...all the other transitions...\\\n",
    "    tran &lt; initial state &gt;  &lt; action taken &gt;  &lt; final state &gt;  &lt; reward &gt;  &lt; transition probability &gt;\\\n",
    "    gamma &lt; discount rate &gt;\n",
    "\n",
    "- Output text file will contain the output in the format:\n",
    "\n",
    "    &lt; optimal value function for first state &gt;  &lt; optimal action for first state &gt;\\\n",
    "    ...one entry for each state...\\\n",
    "    &lt; optimal value function for last state &gt;  &lt; optimal action for last state &gt;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
